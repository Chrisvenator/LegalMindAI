#!/usr/bin/env python3
"""
Austrian Law Text Scraper for ChromaDB RAG System
Downloads and formats Austrian law texts from RIS (ris.bka.gv.at)
"""

import requests
from bs4 import BeautifulSoup
import re
import json
import time
from datetime import datetime
from urllib.parse import urljoin, urlparse
from typing import List, Dict, Optional
import hashlib
import os

class AustrianLawScraper:
    def __init__(self, delay_seconds: float = 1.0):
        """
        Initialize the scraper with rate limiting

        Args:
            delay_seconds: Delay between requests to be respectful to the server
        """
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })
        self.delay = delay_seconds
        self.base_url = "https://www.ris.bka.gv.at"

    def fetch_law_text(self, url: str) -> Optional[Dict]:
        """
        Fetch and parse a single law text from RIS

        Args:
            url: Full URL to the law document

        Returns:
            Dictionary with parsed law data or None if failed
        """
        try:
            print(f"Fetching: {url}")
            response = self.session.get(url, timeout=30)
            response.raise_for_status()

            soup = BeautifulSoup(response.content, 'html.parser')

            # Extract metadata
            law_data = {
                'url': url,
                'scraped_at': datetime.now().isoformat(),
                'document_id': self._extract_document_id(url),
            }

            # Extract title
            title_elem = soup.find('h1') or soup.find('title')
            if title_elem:
                law_data['title'] = self._clean_text(title_elem.get_text())

            # Extract law number/identifier
            law_data['law_number'] = self._extract_law_number(soup, url)

            # Extract the main legal text
            law_data['content'] = self._extract_content(soup)

            # Extract articles/sections
            law_data['sections'] = self._extract_sections(soup)

            # Extract metadata like dates, categories
            law_data['metadata'] = self._extract_metadata(soup)

            # Create document chunks for ChromaDB
            law_data['chunks'] = self._create_chunks(law_data)

            time.sleep(self.delay)  # Rate limiting
            return law_data

        except Exception as e:
            print(f"Error fetching {url}: {str(e)}")
            return None

    def _extract_document_id(self, url: str) -> str:
        """Extract a unique document ID from the URL"""
        # Create hash from URL for unique ID
        return hashlib.md5(url.encode()).hexdigest()[:12]

    def _extract_law_number(self, soup: BeautifulSoup, url: str) -> str:
        """Extract law number/identifier"""
        # Try to find law number in various places
        patterns = [
            r'BGBl\.?\s*I?\s*Nr\.?\s*(\d+/\d+)',
            r'BGBl\.?\s*(\d+/\d+)',
            r'Nr\.?\s*(\d+/\d+)'
        ]

        text = soup.get_text()
        for pattern in patterns:
            match = re.search(pattern, text, re.IGNORECASE)
            if match:
                return match.group(1)

        # Fallback: extract from URL
        url_match = re.search(r'/(\d+/\d+)/', url)
        if url_match:
            return url_match.group(1)

        return "unknown"

    def _extract_content(self, soup: BeautifulSoup) -> str:
        """Extract main legal content"""
        # Remove navigation, headers, footers
        for elem in soup.find_all(['nav', 'header', 'footer', 'script', 'style']):
            elem.decompose()

        # Look for main content areas
        content_selectors = [
            '.law-content',
            '.gesetzestext',
            '.content',
            '#main-content',
            '.main'
        ]

        content = ""
        for selector in content_selectors:
            content_elem = soup.select_one(selector)
            if content_elem:
                content = self._clean_text(content_elem.get_text())
                break

        # Fallback: get all text and clean it
        if not content:
            content = self._clean_text(soup.get_text())

        return content

    def _extract_sections(self, soup: BeautifulSoup) -> List[Dict]:
        """Extract individual sections/articles"""
        sections = []

        # Common patterns for Austrian law sections
        section_patterns = [
            r'§\s*(\d+[a-z]?)\.',  # § 1. § 2a.
            r'Art\.?\s*(\d+)',      # Art. 1, Artikel 1
            r'Abschnitt\s*(\d+)',   # Abschnitt 1
        ]

        text = soup.get_text()

        # Split text by sections
        for pattern in section_patterns:
            matches = list(re.finditer(pattern, text, re.IGNORECASE))
            if matches:
                for i, match in enumerate(matches):
                    start_pos = match.start()
                    end_pos = matches[i + 1].start() if i + 1 < len(matches) else len(text)

                    section_text = text[start_pos:end_pos].strip()
                    if len(section_text) > 50:  # Filter out very short sections
                        sections.append({
                            'section_number': match.group(1),
                            'content': self._clean_text(section_text),
                            'type': 'paragraph' if '§' in pattern else 'article'
                        })
                break

        return sections

    def _extract_metadata(self, soup: BeautifulSoup) -> Dict:
        """Extract metadata like dates, categories, etc."""
        metadata = {}

        # Look for common metadata patterns
        text = soup.get_text()

        # Extract dates
        date_patterns = [
            r'vom\s+(\d{1,2}\.\s*\w+\s+\d{4})',
            r'(\d{1,2}\.\d{1,2}\.\d{4})',
        ]

        for pattern in date_patterns:
            match = re.search(pattern, text)
            if match:
                metadata['date'] = match.group(1)
                break

        # Extract legal categories
        if 'Bundesgesetz' in text:
            metadata['type'] = 'Bundesgesetz'
        elif 'Verordnung' in text:
            metadata['type'] = 'Verordnung'
        elif 'Richtlinie' in text:
            metadata['type'] = 'Richtlinie'

        return metadata

    def _clean_text(self, text: str) -> str:
        """Clean and normalize text"""
        # Remove extra whitespace
        text = re.sub(r'\s+', ' ', text)
        # Remove special characters that might cause issues
        text = re.sub(r'[\x00-\x08\x0b\x0c\x0e-\x1f\x7f-\x9f]', '', text)
        return text.strip()

    def _create_chunks(self, law_data: Dict) -> List[Dict]:
        """Create chunks suitable for ChromaDB ingestion"""
        chunks = []

        # Create a chunk for the full document
        full_chunk = {
            'id': f"{law_data['document_id']}_full",
            'text': f"Title: {law_data.get('title', '')}\n\n{law_data['content']}",
            'metadata': {
                'document_id': law_data['document_id'],
                'url': law_data['url'],
                'title': law_data.get('title', ''),
                'law_number': law_data['law_number'],
                'chunk_type': 'full_document',
                'scraped_at': law_data['scraped_at'],
                **law_data.get('metadata', {})
            }
        }
        chunks.append(full_chunk)

        # Create chunks for individual sections
        for i, section in enumerate(law_data.get('sections', [])):
            section_chunk = {
                'id': f"{law_data['document_id']}_section_{i}",
                'text': section['content'],
                'metadata': {
                    'document_id': law_data['document_id'],
                    'url': law_data['url'],
                    'title': law_data.get('title', ''),
                    'law_number': law_data['law_number'],
                    'section_number': section.get('section_number', ''),
                    'chunk_type': 'section',
                    'section_type': section.get('type', ''),
                    'scraped_at': law_data['scraped_at'],
                    **law_data.get('metadata', {})
                }
            }
            chunks.append(section_chunk)

        # If no sections found, create smaller chunks from content
        if not law_data.get('sections'):
            content = law_data['content']
            chunk_size = 1000  # Characters per chunk
            overlap = 200      # Character overlap between chunks

            for i in range(0, len(content), chunk_size - overlap):
                chunk_text = content[i:i + chunk_size]
                if len(chunk_text.strip()) > 100:  # Only create meaningful chunks
                    text_chunk = {
                        'id': f"{law_data['document_id']}_chunk_{i//chunk_size}",
                        'text': chunk_text,
                        'metadata': {
                            'document_id': law_data['document_id'],
                            'url': law_data['url'],
                            'title': law_data.get('title', ''),
                            'law_number': law_data['law_number'],
                            'chunk_type': 'content_chunk',
                            'chunk_index': i // chunk_size,
                            'scraped_at': law_data['scraped_at'],
                            **law_data.get('metadata', {})
                        }
                    }
                    chunks.append(text_chunk)

        return chunks

    def save_to_chromadb_format(self, law_data_list: List[Dict], output_file: str):
        """Save processed law data in format suitable for ChromaDB"""
        all_chunks = []

        for law_data in law_data_list:
            if law_data and 'chunks' in law_data:
                all_chunks.extend(law_data['chunks'])

        # Prepare data for ChromaDB
        chromadb_data = {
            'documents': [chunk['text'] for chunk in all_chunks],
            'metadatas': [chunk['metadata'] for chunk in all_chunks],
            'ids': [chunk['id'] for chunk in all_chunks]
        }

        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(chromadb_data, f, ensure_ascii=False, indent=2)

        print(f"Saved {len(all_chunks)} chunks to {output_file}")

    def save_as_text_files(self, law_data_list: List[Dict], output_dir: str = "./resources"):
        """Save processed law data as text files compatible with existing app.py runSetup"""
        os.makedirs(output_dir, exist_ok=True)

        for law_data in law_data_list:
            if not law_data:
                continue

            # Create filename from law number and title
            law_number = law_data.get('law_number', 'unknown')
            title = law_data.get('title', 'untitled')
            # Clean filename
            filename = f"{law_number}_{title}"
            filename = re.sub(r'[^\w\s-]', '', filename)
            filename = re.sub(r'[-\s]+', '_', filename)
            filename = filename[:100] + '.txt'  # Limit length

            filepath = os.path.join(output_dir, filename)

            with open(filepath, 'w', encoding='utf-8') as f:
                # Write title
                f.write(f"TITLE: {law_data.get('title', '')}\n")
                f.write(f"LAW NUMBER: {law_data.get('law_number', '')}\n")
                f.write(f"URL: {law_data.get('url', '')}\n")
                f.write("="*50 + "\n\n")

                # Write sections as paragraphs (compatible with your paragraph splitting)
                if law_data.get('sections'):
                    for section in law_data['sections']:
                        section_header = f"§ {section.get('section_number', '')} - {section.get('type', '')}"
                        f.write(f"{section_header}\n\n")
                        f.write(f"{section['content']}\n\n")
                else:
                    # Write content split into chunks
                    content = law_data.get('content', '')
                    # Split into reasonable paragraphs
                    paragraphs = content.split('\n')
                    current_paragraph = ""

                    for line in paragraphs:
                        line = line.strip()
                        if not line:
                            if current_paragraph:
                                f.write(f"{current_paragraph.strip()}\n\n")
                                current_paragraph = ""
                        else:
                            current_paragraph += line + " "
                            # If paragraph gets too long, break it
                            if len(current_paragraph) > 1000:
                                f.write(f"{current_paragraph.strip()}\n\n")
                                current_paragraph = ""

                    if current_paragraph:
                        f.write(f"{current_paragraph.strip()}\n\n")

            print(f"Saved law text to: {filepath}")

        print(f"All law texts saved to {output_dir}/ directory")

    def scrape_multiple_laws(self, urls: List[str], output_file: str = "austrian_laws.json", save_as_txt: bool = False):
        """Scrape multiple law documents"""
        law_data_list = []

        for url in urls:
            law_data = self.fetch_law_text(url)
            if law_data:
                law_data_list.append(law_data)
            else:
                print(f"Failed to process: {url}")

        # Save in ChromaDB format
        self.save_to_chromadb_format(law_data_list, output_file)

        # Optionally save as text files for existing app.py compatibility
        if save_as_txt:
            self.save_as_text_files(law_data_list)

        # Also save raw data for debugging
        raw_output = output_file.replace('.json', '_raw.json')
        with open(raw_output, 'w', encoding='utf-8') as f:
            json.dump(law_data_list, f, ensure_ascii=False, indent=2)

        return law_data_list

# Example usage
if __name__ == "__main__":
    # Initialize scraper
    scraper = AustrianLawScraper(delay_seconds=1.0)

    # Example URLs (add more as needed)
    law_urls = [
        "https://www.ris.bka.gv.at/eli/bgbl/i/1999/165/P0/NOR40212004",
        # Add more Austrian law URLs here
    ]

    # Option 1: Save as text files for your existing app.py runSetup method
    print("Scraping laws and saving as text files for existing app.py...")
    scraper.scrape_multiple_laws(
        law_urls,
        "austrian_laws_chromadb.json",
        save_as_txt=True  # This will save to ./resources/ directory
    )

    print(f"Successfully processed {len(law_urls)} law documents")
    print("Files created:")
    print("- ./resources/*.txt files (for your existing runSetup method)")
    print("- austrian_laws_chromadb.json (for direct ChromaDB ingestion)")
    print("\nNow you can run your app.py runSetup() to load the law texts!")

# ChromaDB Integration Example
"""
To use with ChromaDB:

import chromadb
import json

# Load the processed data
with open('austrian_laws_chromadb.json', 'r', encoding='utf-8') as f:
    data = json.load(f)

# Initialize ChromaDB
client = chromadb.Client()
collection = client.create_collection(name="austrian_laws")

# Add documents to ChromaDB
collection.add(
    documents=data['documents'],
    metadatas=data['metadatas'],
    ids=data['ids']
)

# Query example
results = collection.query(
    query_texts=["Datenschutz"],
    n_results=5
)
"""